{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Current device: 0\n",
      "Device: cuda:0\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\") \n",
    "device_count = torch.cuda.device_count() \n",
    "device_name =  torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f'Cuda available: {available}')\n",
    "print(f'Current device: {curr_device}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Device count: {device_count}')\n",
    "print(f'Device name: {device_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# returns {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 102, 102, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], 'length': [10], 'overflow_to_sample_mapping': [0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus('[SEP]', \n",
    "                      add_special_tokens=True,\n",
    "                      padding='max_length',\n",
    "                      truncation=True,\n",
    "                      max_length=10,\n",
    "                      return_token_type_ids=False,\n",
    "                      return_overflowing_tokens=True,\n",
    "                      return_length=True,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31264/3321853083.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nn.functional.softmax(c)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([0.25, 0.25, 0.25, 0.25])\n",
    "b = (1.0 - torch.Tensor([1, 1, 0, 0])) * -10000\n",
    "c = a + b\n",
    "nn.functional.softmax(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "wikifile = '/home/bjlkeng/devel/cramming/data/wikipedia/wikipedia-en-0.json'\n",
    "\n",
    "with open(wikifile, 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603315"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDatasetError",
     "evalue": "The directory at /home/bjlkeng/devel/cramming/data/**.txt doesn't contain any data files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDatasetError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     files \u001b[39m=\u001b[39m [prefix \u001b[39m+\u001b[39m fname\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m fname \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mreadlines()]\n\u001b[1;32m      7\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/bjlkeng/devel/cramming/data/**.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m, data_dir\u001b[39m=\u001b[39;49mdata_dir, sample_by\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m'\u001b[39;49m, streaming\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m \u001b[39m#except Exception as e:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#    print(e, '\\n', i, files[i*batch:(i+1)*batch])\u001b[39;00m\n",
      "File \u001b[0;32m~/devel/cramming/.conda/lib/python3.10/site-packages/datasets/load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1755\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1756\u001b[0m )\n\u001b[1;32m   1758\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1760\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1761\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1762\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1763\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1764\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1765\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1766\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1767\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1768\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1769\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1770\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1771\u001b[0m )\n\u001b[1;32m   1773\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/devel/cramming/.conda/lib/python3.10/site-packages/datasets/load.py:1496\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1495\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1496\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1497\u001b[0m     path,\n\u001b[1;32m   1498\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1499\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1500\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1501\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1502\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1503\u001b[0m )\n\u001b[1;32m   1505\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/devel/cramming/.conda/lib/python3.10/site-packages/datasets/load.py:1135\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[39m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \n\u001b[1;32m   1127\u001b[0m \u001b[39m# Try packaged\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mreturn\u001b[39;00m PackagedDatasetModuleFactory(\n\u001b[1;32m   1130\u001b[0m         path,\n\u001b[1;32m   1131\u001b[0m         data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1132\u001b[0m         data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1133\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1134\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m-> 1135\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1136\u001b[0m \u001b[39m# Try locally\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[39melif\u001b[39;00m path\u001b[39m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m~/devel/cramming/.conda/lib/python3.10/site-packages/datasets/load.py:707\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_module\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetModule:\n\u001b[1;32m    705\u001b[0m     base_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir)\u001b[39m.\u001b[39mresolve()) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(Path()\u001b[39m.\u001b[39mresolve())\n\u001b[1;32m    706\u001b[0m     patterns \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 707\u001b[0m         sanitize_patterns(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m get_data_patterns_locally(base_path)\n\u001b[1;32m    708\u001b[0m     )\n\u001b[1;32m    709\u001b[0m     data_files \u001b[39m=\u001b[39m DataFilesDict\u001b[39m.\u001b[39mfrom_local_or_remote(\n\u001b[1;32m    710\u001b[0m         patterns,\n\u001b[1;32m    711\u001b[0m         use_auth_token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_config\u001b[39m.\u001b[39muse_auth_token,\n\u001b[1;32m    712\u001b[0m         base_path\u001b[39m=\u001b[39mbase_path,\n\u001b[1;32m    713\u001b[0m     )\n\u001b[1;32m    714\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m _MODULE_SUPPORTS_METADATA \u001b[39mand\u001b[39;00m patterns \u001b[39m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m~/devel/cramming/.conda/lib/python3.10/site-packages/datasets/data_files.py:461\u001b[0m, in \u001b[0;36mget_data_patterns_locally\u001b[0;34m(base_path)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_data_files_patterns(resolver)\n\u001b[1;32m    460\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     \u001b[39mraise\u001b[39;00m EmptyDatasetError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe directory at \u001b[39m\u001b[39m{\u001b[39;00mbase_path\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain any data files\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mEmptyDatasetError\u001b[0m: The directory at /home/bjlkeng/devel/cramming/data/**.txt doesn't contain any data files"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "filelist = '/home/bjlkeng/devel/cramming/data/files.csv'\n",
    "with open(filelist, \"r\") as f:\n",
    "    files = [prefix + fname.strip() for fname in f.readlines()]\n",
    "\n",
    "data_dir = '/home/bjlkeng/devel/cramming/data/'\n",
    "dataset = load_dataset('text', data_dir=data_dir, sample_by='document', streaming=True)\n",
    "#except Exception as e:\n",
    "#    print(e, '\\n', i, files[i*batch:(i+1)*batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'MAYAN DECEMBER\\n', '\\n', 'Brenda Cooper\\n', '\\n', 'To Russell Patrick Cooper  \\n', 'Good journeys, little brother\\n', '\\n', 'Copyright © 2011 by Brenda Cooper.\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(files[287], 'r') as f:\n",
    "    print(f.readlines()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <datasets.iterable_dataset.IterableDataset at 0x7f6abd906d40>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    input_ids = []\n",
    "    for example in examples['text']:\n",
    "        tokens = tokenizer.encode_plus(example, \n",
    "                                       add_special_tokens=True,\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True,\n",
    "                                       max_length=128,\n",
    "                                       return_token_type_ids=False,\n",
    "                                       return_overflowing_tokens=True,)\n",
    "        input_ids += tokens['input_ids']\n",
    "    return {'input_ids': input_ids}\n",
    "\n",
    "dataset2 = dataset.map(tokenize, batched=True, remove_columns=['text'], batch_size=1000)\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset2['train'][0]['input_ids']\n",
    "\n",
    "batch = list(dataset2['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  4067,\n",
       "  2017,\n",
       "  2005,\n",
       "  8816,\n",
       "  2075,\n",
       "  2023,\n",
       "  4079,\n",
       "  1004,\n",
       "  24253,\n",
       "  26885,\n",
       "  1012,\n",
       "  1008,\n",
       "  1008,\n",
       "  1008,\n",
       "  2131,\n",
       "  1037,\n",
       "  2489,\n",
       "  26885,\n",
       "  2043,\n",
       "  2017,\n",
       "  3693,\n",
       "  2256,\n",
       "  5653,\n",
       "  2075,\n",
       "  2862,\n",
       "  1012,\n",
       "  4606,\n",
       "  1010,\n",
       "  2131,\n",
       "  14409,\n",
       "  2006,\n",
       "  2047,\n",
       "  7085,\n",
       "  1010,\n",
       "  9144,\n",
       "  1010,\n",
       "  6749,\n",
       "  9631,\n",
       "  1010,\n",
       "  1998,\n",
       "  2062,\n",
       "  2013,\n",
       "  4079,\n",
       "  1004,\n",
       "  24253,\n",
       "  1012,\n",
       "  11562,\n",
       "  2917,\n",
       "  2000,\n",
       "  3696,\n",
       "  2039,\n",
       "  1998,\n",
       "  2156,\n",
       "  3408,\n",
       "  1998,\n",
       "  3785,\n",
       "  1012,\n",
       "  11562,\n",
       "  2182,\n",
       "  2000,\n",
       "  3696,\n",
       "  2039,\n",
       "  2525,\n",
       "  1037,\n",
       "  4942,\n",
       "  29234,\n",
       "  2099,\n",
       "  1029,\n",
       "  3073,\n",
       "  2115,\n",
       "  10373,\n",
       "  2153,\n",
       "  2061,\n",
       "  2057,\n",
       "  2064,\n",
       "  4236,\n",
       "  2023,\n",
       "  26885,\n",
       "  1998,\n",
       "  4604,\n",
       "  2017,\n",
       "  2062,\n",
       "  1997,\n",
       "  2054,\n",
       "  2017,\n",
       "  2066,\n",
       "  2000,\n",
       "  3191,\n",
       "  1012,\n",
       "  2017,\n",
       "  2097,\n",
       "  3613,\n",
       "  2000,\n",
       "  4374,\n",
       "  7262,\n",
       "  4107,\n",
       "  1999,\n",
       "  2115,\n",
       "  1999,\n",
       "  8758,\n",
       "  1012,\n",
       "  1001,\n",
       "  1001,\n",
       "  8417,\n",
       "  4958,\n",
       "  8004,\n",
       "  24342,\n",
       "  3166,\n",
       "  1005,\n",
       "  1055,\n",
       "  3602,\n",
       "  18877,\n",
       "  2190,\n",
       "  7226,\n",
       "  3127,\n",
       "  1015,\n",
       "  6315,\n",
       "  3430,\n",
       "  3127,\n",
       "  1016,\n",
       "  28618,\n",
       "  6277,\n",
       "  17326,\n",
       "  3127,\n",
       "  1017,\n",
       "  2619,\n",
       "  102]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(batch))\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b020ec3df0c18b68bf7b4552e70845f81858dcbaf541b576d2fc743ffe38c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
