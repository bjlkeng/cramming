{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Current device: 0\n",
      "Device: cuda:0\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\") \n",
    "device_count = torch.cuda.device_count() \n",
    "device_name =  torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f'Cuda available: {available}')\n",
    "print(f'Current device: {curr_device}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Device count: {device_count}')\n",
    "print(f'Device name: {device_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# returns {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 102, 102, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], 'length': [10], 'overflow_to_sample_mapping': [0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus('[SEP]', \n",
    "                      add_special_tokens=True,\n",
    "                      padding='max_length',\n",
    "                      truncation=True,\n",
    "                      max_length=10,\n",
    "                      return_token_type_ids=False,\n",
    "                      return_overflowing_tokens=True,\n",
    "                      return_length=True,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31264/3321853083.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nn.functional.softmax(c)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([0.25, 0.25, 0.25, 0.25])\n",
    "b = (1.0 - torch.Tensor([1, 1, 0, 0])) * -10000\n",
    "c = a + b\n",
    "nn.functional.softmax(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "wikifile = '/home/bjlkeng/devel/cramming/data/wikipedia/wikipedia-en-0.json'\n",
    "\n",
    "with open(wikifile, 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603315"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 29/29 [00:00<00:00, 246224.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "files = [\"A $500 House in Detroit - Drew Philp.epub.txt\",\n",
    "         \"A & C Black - Negotiate Successfully [retail].epub.txt\",\n",
    "         \"A 'New You' for Your 'New Baby' - Preeti Subberwal.epub.txt\",\n",
    "         \"A - Tom Bullough.epub.txt\",\n",
    "         \"A 14-Day Romance Challenge - Sharon Jaynes.epub.txt\",\n",
    "         \"A 1950s Housewife - Sheila Hardy.epub.txt\",\n",
    "         \"A 1960s Childhood - Paul Feeney.epub.txt\",\n",
    "         \"A 1970s Teenager - Webb, Simon;.epub.txt\",\n",
    "         \"A 1990s Childhood - Michael Johnson.epub.txt\",\n",
    "         \"A 30 Years' War_ A Study of and - Thomas Ellsworth Walker.epub.txt\",]\n",
    "prefix = \"/home/bjlkeng/devel/cramming/data/books3/books3/the-eye.eu/public/Books/Bibliotik/A/\"\n",
    "files = [prefix + f for f in files]\n",
    "\n",
    "files2 = [\n",
    "\"10.txt\",\n",
    "\"100.txt\",\n",
    "\"10002.txt\",\n",
    "\"10003.txt\",\n",
    "\"10004.txt\",\n",
    "\"10005.txt\",\n",
    "\"10006.txt\",\n",
    "\"10007.txt\",\n",
    "\"10008.txt\",\n",
    "\"10009.txt\",\n",
    "]\n",
    "prefix2 = '/home/bjlkeng/devel/cramming/data/pg19_train/'\n",
    "files += [prefix2 + f for f in files2]\n",
    "\n",
    "files3 = [\n",
    "\"wikifile-0-0.txt\",\n",
    "\"wikifile-0-1000.txt\",\n",
    "\"wikifile-0-10000.txt\",\n",
    "\"wikifile-0-100000.txt\",\n",
    "\"wikifile-0-100200.txt\",\n",
    "\"wikifile-0-100400.txt\",\n",
    "\"wikifile-0-100600.txt\",\n",
    "\"wikifile-0-100800.txt\",\n",
    "\"wikifile-0-101000.txt\",\n",
    "\"wikifile-0-101200.txt\",\n",
    "]\n",
    "prefix3 = '/home/bjlkeng/devel/cramming/data/wikipedia/flat/'\n",
    "files += [prefix3 + f for f in files3]\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=files, sample_by='document', streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <datasets.iterable_dataset.IterableDataset at 0x7f6abd906d40>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    input_ids = []\n",
    "    for example in examples['text']:\n",
    "        tokens = tokenizer.encode_plus(example, \n",
    "                                       add_special_tokens=True,\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True,\n",
    "                                       max_length=128,\n",
    "                                       return_token_type_ids=False,\n",
    "                                       return_overflowing_tokens=True,)\n",
    "        input_ids += tokens['input_ids']\n",
    "    return {'input_ids': input_ids}\n",
    "\n",
    "dataset2 = dataset.map(tokenize, batched=True, remove_columns=['text'], batch_size=1000)\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset2['train'][0]['input_ids']\n",
    "\n",
    "batch = list(dataset2['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  4067,\n",
       "  2017,\n",
       "  2005,\n",
       "  8816,\n",
       "  2075,\n",
       "  2023,\n",
       "  4079,\n",
       "  1004,\n",
       "  24253,\n",
       "  26885,\n",
       "  1012,\n",
       "  1008,\n",
       "  1008,\n",
       "  1008,\n",
       "  2131,\n",
       "  1037,\n",
       "  2489,\n",
       "  26885,\n",
       "  2043,\n",
       "  2017,\n",
       "  3693,\n",
       "  2256,\n",
       "  5653,\n",
       "  2075,\n",
       "  2862,\n",
       "  1012,\n",
       "  4606,\n",
       "  1010,\n",
       "  2131,\n",
       "  14409,\n",
       "  2006,\n",
       "  2047,\n",
       "  7085,\n",
       "  1010,\n",
       "  9144,\n",
       "  1010,\n",
       "  6749,\n",
       "  9631,\n",
       "  1010,\n",
       "  1998,\n",
       "  2062,\n",
       "  2013,\n",
       "  4079,\n",
       "  1004,\n",
       "  24253,\n",
       "  1012,\n",
       "  11562,\n",
       "  2917,\n",
       "  2000,\n",
       "  3696,\n",
       "  2039,\n",
       "  1998,\n",
       "  2156,\n",
       "  3408,\n",
       "  1998,\n",
       "  3785,\n",
       "  1012,\n",
       "  11562,\n",
       "  2182,\n",
       "  2000,\n",
       "  3696,\n",
       "  2039,\n",
       "  2525,\n",
       "  1037,\n",
       "  4942,\n",
       "  29234,\n",
       "  2099,\n",
       "  1029,\n",
       "  3073,\n",
       "  2115,\n",
       "  10373,\n",
       "  2153,\n",
       "  2061,\n",
       "  2057,\n",
       "  2064,\n",
       "  4236,\n",
       "  2023,\n",
       "  26885,\n",
       "  1998,\n",
       "  4604,\n",
       "  2017,\n",
       "  2062,\n",
       "  1997,\n",
       "  2054,\n",
       "  2017,\n",
       "  2066,\n",
       "  2000,\n",
       "  3191,\n",
       "  1012,\n",
       "  2017,\n",
       "  2097,\n",
       "  3613,\n",
       "  2000,\n",
       "  4374,\n",
       "  7262,\n",
       "  4107,\n",
       "  1999,\n",
       "  2115,\n",
       "  1999,\n",
       "  8758,\n",
       "  1012,\n",
       "  1001,\n",
       "  1001,\n",
       "  8417,\n",
       "  4958,\n",
       "  8004,\n",
       "  24342,\n",
       "  3166,\n",
       "  1005,\n",
       "  1055,\n",
       "  3602,\n",
       "  18877,\n",
       "  2190,\n",
       "  7226,\n",
       "  3127,\n",
       "  1015,\n",
       "  6315,\n",
       "  3430,\n",
       "  3127,\n",
       "  1016,\n",
       "  28618,\n",
       "  6277,\n",
       "  17326,\n",
       "  3127,\n",
       "  1017,\n",
       "  2619,\n",
       "  102]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(batch))\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b020ec3df0c18b68bf7b4552e70845f81858dcbaf541b576d2fc743ffe38c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
