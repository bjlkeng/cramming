{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.dataloaders import CoLADataModule\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Current device: 0\n",
      "Device: cuda:0\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\") \n",
    "device_count = torch.cuda.device_count() \n",
    "device_name =  torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f'Cuda available: {available}')\n",
    "print(f'Current device: {curr_device}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Device count: {device_count}')\n",
    "print(f'Device name: {device_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = SentenceTransformer('all-mpnet-base-v2')\n",
    "dm = CoLADataModule(data_dir='./glue_data/CoLA/', batch_size=1000, sentence_encoder=sentence_encoder)\n",
    "\n",
    "#dm.setup(stage='fit')\n",
    "#dm.setup(stage='validate')\n",
    "#dm.setup(stage='test')\n",
    "#\n",
    "#for i, (x, y) in enumerate(dm.train):\n",
    "#    if i < 10:\n",
    "#        print(f'i: x = {x}, y = {y}')\n",
    "\n",
    "#sentence_encoder.encode(dm.train.x_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightningModule\n",
    "class CoLAClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(nn.Linear(768, 768), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(768, 1),\n",
    "                                     nn.Sigmoid())\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.network(x)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, targets = batch\n",
    "        y = self.network(x)\n",
    "        print(x.shape, y.shape)\n",
    "\n",
    "        loss = nn.functional.binary_cross_entropy(y, targets)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, targets = batch\n",
    "        y = self.network(x)\n",
    "        val_loss = nn.functional.binary_cross_entropy(y, targets)\n",
    "        self.log(\"test_accuracy\", self.accuracy(y, targets))\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, targets = batch\n",
    "        y = self.network(x)\n",
    "        metric = BinaryAccuracy()\n",
    "        self.log(\"test_accuracy\", self.accuracy(y, targets))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "cls = CoLAClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type           | Params\n",
      "--------------------------------------------\n",
      "0 | network  | Sequential     | 591 K \n",
      "1 | accuracy | BinaryAccuracy | 0     \n",
      "--------------------------------------------\n",
      "591 K     Trainable params\n",
      "0         Non-trainable params\n",
      "591 K     Total params\n",
      "2.365     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/11 [00:00<?, ?it/s] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:   9%|▉         | 1/11 [00:00<00:00, 127.43it/s, loss=0.686, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:  18%|█▊        | 2/11 [00:00<00:00, 141.25it/s, loss=0.683, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:  27%|██▋       | 3/11 [00:00<00:00, 149.85it/s, loss=0.678, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:  36%|███▋      | 4/11 [00:00<00:00, 150.41it/s, loss=0.671, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:  45%|████▌     | 5/11 [00:00<00:00, 149.25it/s, loss=0.668, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:  55%|█████▍    | 6/11 [00:00<00:00, 150.43it/s, loss=0.661, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:  64%|██████▎   | 7/11 [00:00<00:00, 148.02it/s, loss=0.653, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 0:  73%|███████▎  | 8/11 [00:00<00:00, 146.51it/s, loss=0.648, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 1:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.647, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:   9%|▉         | 1/11 [00:00<00:00, 109.90it/s, loss=0.647, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:  18%|█▊        | 2/11 [00:00<00:00, 121.94it/s, loss=0.648, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:  27%|██▋       | 3/11 [00:00<00:00, 130.05it/s, loss=0.647, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:  36%|███▋      | 4/11 [00:00<00:00, 130.71it/s, loss=0.643, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:  45%|████▌     | 5/11 [00:00<00:00, 129.75it/s, loss=0.642, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:  55%|█████▍    | 6/11 [00:00<00:00, 134.96it/s, loss=0.637, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:  64%|██████▎   | 7/11 [00:00<00:00, 138.84it/s, loss=0.632, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 1:  73%|███████▎  | 8/11 [00:00<00:00, 140.91it/s, loss=0.63, v_num=35] torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 2:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.63, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:   9%|▉         | 1/11 [00:00<00:00, 123.26it/s, loss=0.632, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:  18%|█▊        | 2/11 [00:00<00:00, 134.12it/s, loss=0.634, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:  27%|██▋       | 3/11 [00:00<00:00, 141.50it/s, loss=0.631, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:  36%|███▋      | 4/11 [00:00<00:00, 145.26it/s, loss=0.626, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:  45%|████▌     | 5/11 [00:00<00:00, 146.82it/s, loss=0.624, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:  55%|█████▍    | 6/11 [00:00<00:00, 147.84it/s, loss=0.62, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:  64%|██████▎   | 7/11 [00:00<00:00, 149.26it/s, loss=0.614, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 2:  73%|███████▎  | 8/11 [00:00<00:00, 150.15it/s, loss=0.612, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 3:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.612, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:   9%|▉         | 1/11 [00:00<00:00, 119.25it/s, loss=0.614, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:  18%|█▊        | 2/11 [00:00<00:00, 130.69it/s, loss=0.615, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:  27%|██▋       | 3/11 [00:00<00:00, 142.12it/s, loss=0.613, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:  36%|███▋      | 4/11 [00:00<00:00, 143.25it/s, loss=0.609, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:  45%|████▌     | 5/11 [00:00<00:00, 142.71it/s, loss=0.609, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:  55%|█████▍    | 6/11 [00:00<00:00, 142.64it/s, loss=0.607, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:  64%|██████▎   | 7/11 [00:00<00:00, 145.21it/s, loss=0.602, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 3:  73%|███████▎  | 8/11 [00:00<00:00, 146.11it/s, loss=0.603, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 4:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.606, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:   9%|▉         | 1/11 [00:00<00:00, 145.05it/s, loss=0.608, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:  18%|█▊        | 2/11 [00:00<00:00, 148.11it/s, loss=0.609, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:  27%|██▋       | 3/11 [00:00<00:00, 153.16it/s, loss=0.606, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:  36%|███▋      | 4/11 [00:00<00:00, 153.77it/s, loss=0.6, v_num=35]  torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:  45%|████▌     | 5/11 [00:00<00:00, 153.73it/s, loss=0.6, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:  55%|█████▍    | 6/11 [00:00<00:00, 154.48it/s, loss=0.598, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:  64%|██████▎   | 7/11 [00:00<00:00, 155.06it/s, loss=0.593, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 4:  73%|███████▎  | 8/11 [00:00<00:00, 155.07it/s, loss=0.594, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 5:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.596, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:   9%|▉         | 1/11 [00:00<00:00, 135.27it/s, loss=0.599, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:  18%|█▊        | 2/11 [00:00<00:00, 145.81it/s, loss=0.601, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:  27%|██▋       | 3/11 [00:00<00:00, 151.05it/s, loss=0.598, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:  36%|███▋      | 4/11 [00:00<00:00, 154.37it/s, loss=0.594, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:  45%|████▌     | 5/11 [00:00<00:00, 156.52it/s, loss=0.594, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:  55%|█████▍    | 6/11 [00:00<00:00, 158.04it/s, loss=0.591, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:  64%|██████▎   | 7/11 [00:00<00:00, 158.07it/s, loss=0.586, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 5:  73%|███████▎  | 8/11 [00:00<00:00, 158.99it/s, loss=0.586, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 6:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.588, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:   9%|▉         | 1/11 [00:00<00:00, 136.27it/s, loss=0.591, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:  18%|█▊        | 2/11 [00:00<00:00, 143.86it/s, loss=0.593, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:  27%|██▋       | 3/11 [00:00<00:00, 146.00it/s, loss=0.591, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:  36%|███▋      | 4/11 [00:00<00:00, 150.68it/s, loss=0.586, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:  45%|████▌     | 5/11 [00:00<00:00, 152.42it/s, loss=0.586, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:  55%|█████▍    | 6/11 [00:00<00:00, 151.67it/s, loss=0.584, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:  64%|██████▎   | 7/11 [00:00<00:00, 150.94it/s, loss=0.578, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 6:  73%|███████▎  | 8/11 [00:00<00:00, 150.97it/s, loss=0.579, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 7:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.581, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:   9%|▉         | 1/11 [00:00<00:00, 137.50it/s, loss=0.584, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:  18%|█▊        | 2/11 [00:00<00:00, 137.99it/s, loss=0.586, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:  27%|██▋       | 3/11 [00:00<00:00, 140.13it/s, loss=0.584, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:  36%|███▋      | 4/11 [00:00<00:00, 34.98it/s, loss=0.578, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:  45%|████▌     | 5/11 [00:00<00:00, 41.33it/s, loss=0.578, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:  55%|█████▍    | 6/11 [00:00<00:00, 46.97it/s, loss=0.576, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:  64%|██████▎   | 7/11 [00:00<00:00, 52.26it/s, loss=0.57, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 7:  73%|███████▎  | 8/11 [00:00<00:00, 56.94it/s, loss=0.571, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 8:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.574, v_num=35]         torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:   9%|▉         | 1/11 [00:00<00:00, 132.55it/s, loss=0.577, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:  18%|█▊        | 2/11 [00:00<00:00, 136.82it/s, loss=0.579, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:  27%|██▋       | 3/11 [00:00<00:00, 144.58it/s, loss=0.576, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:  36%|███▋      | 4/11 [00:00<00:00, 150.07it/s, loss=0.571, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:  45%|████▌     | 5/11 [00:00<00:00, 149.88it/s, loss=0.571, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:  55%|█████▍    | 6/11 [00:00<00:00, 150.29it/s, loss=0.569, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:  64%|██████▎   | 7/11 [00:00<00:00, 150.73it/s, loss=0.563, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 8:  73%|███████▎  | 8/11 [00:00<00:00, 151.62it/s, loss=0.564, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 9:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.566, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:   9%|▉         | 1/11 [00:00<00:00, 135.40it/s, loss=0.569, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:  18%|█▊        | 2/11 [00:00<00:00, 145.23it/s, loss=0.572, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:  27%|██▋       | 3/11 [00:00<00:00, 150.20it/s, loss=0.569, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:  36%|███▋      | 4/11 [00:00<00:00, 152.97it/s, loss=0.563, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:  45%|████▌     | 5/11 [00:00<00:00, 155.89it/s, loss=0.564, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:  55%|█████▍    | 6/11 [00:00<00:00, 157.92it/s, loss=0.562, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:  64%|██████▎   | 7/11 [00:00<00:00, 159.66it/s, loss=0.556, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 9:  73%|███████▎  | 8/11 [00:00<00:00, 159.30it/s, loss=0.557, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 10:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.559, v_num=35]         torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:   9%|▉         | 1/11 [00:00<00:00, 119.83it/s, loss=0.562, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:  18%|█▊        | 2/11 [00:00<00:00, 138.51it/s, loss=0.565, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:  27%|██▋       | 3/11 [00:00<00:00, 143.45it/s, loss=0.562, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:  36%|███▋      | 4/11 [00:00<00:00, 148.75it/s, loss=0.556, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:  45%|████▌     | 5/11 [00:00<00:00, 151.89it/s, loss=0.557, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:  55%|█████▍    | 6/11 [00:00<00:00, 154.37it/s, loss=0.555, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:  64%|██████▎   | 7/11 [00:00<00:00, 154.30it/s, loss=0.549, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 10:  73%|███████▎  | 8/11 [00:00<00:00, 154.78it/s, loss=0.55, v_num=35] torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 11:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.552, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:   9%|▉         | 1/11 [00:00<00:00, 142.92it/s, loss=0.555, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:  18%|█▊        | 2/11 [00:00<00:00, 147.46it/s, loss=0.558, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:  27%|██▋       | 3/11 [00:00<00:00, 152.06it/s, loss=0.555, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:  36%|███▋      | 4/11 [00:00<00:00, 154.32it/s, loss=0.549, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:  45%|████▌     | 5/11 [00:00<00:00, 156.22it/s, loss=0.55, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:  55%|█████▍    | 6/11 [00:00<00:00, 158.36it/s, loss=0.548, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:  64%|██████▎   | 7/11 [00:00<00:00, 157.31it/s, loss=0.542, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 11:  73%|███████▎  | 8/11 [00:00<00:00, 157.49it/s, loss=0.543, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 12:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.545, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:   9%|▉         | 1/11 [00:00<00:00, 125.65it/s, loss=0.548, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:  18%|█▊        | 2/11 [00:00<00:00, 135.75it/s, loss=0.551, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:  27%|██▋       | 3/11 [00:00<00:00, 140.14it/s, loss=0.548, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:  36%|███▋      | 4/11 [00:00<00:00, 146.51it/s, loss=0.542, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:  45%|████▌     | 5/11 [00:00<00:00, 149.32it/s, loss=0.543, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:  55%|█████▍    | 6/11 [00:00<00:00, 152.74it/s, loss=0.541, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:  64%|██████▎   | 7/11 [00:00<00:00, 153.01it/s, loss=0.535, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 12:  73%|███████▎  | 8/11 [00:00<00:00, 155.10it/s, loss=0.535, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 13:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.538, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:   9%|▉         | 1/11 [00:00<00:00, 135.59it/s, loss=0.541, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:  18%|█▊        | 2/11 [00:00<00:00, 147.16it/s, loss=0.544, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:  27%|██▋       | 3/11 [00:00<00:00, 150.54it/s, loss=0.541, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:  36%|███▋      | 4/11 [00:00<00:00, 154.35it/s, loss=0.535, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:  45%|████▌     | 5/11 [00:00<00:00, 153.78it/s, loss=0.536, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:  55%|█████▍    | 6/11 [00:00<00:00, 156.18it/s, loss=0.534, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:  64%|██████▎   | 7/11 [00:00<00:00, 153.80it/s, loss=0.528, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 13:  73%|███████▎  | 8/11 [00:00<00:00, 154.49it/s, loss=0.528, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 14:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.531, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:   9%|▉         | 1/11 [00:00<00:00, 141.39it/s, loss=0.534, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:  18%|█▊        | 2/11 [00:00<00:00, 144.32it/s, loss=0.537, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:  27%|██▋       | 3/11 [00:00<00:00, 151.28it/s, loss=0.534, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:  36%|███▋      | 4/11 [00:00<00:00, 156.30it/s, loss=0.528, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:  45%|████▌     | 5/11 [00:00<00:00, 156.59it/s, loss=0.528, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:  55%|█████▍    | 6/11 [00:00<00:00, 154.24it/s, loss=0.527, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:  64%|██████▎   | 7/11 [00:00<00:00, 153.82it/s, loss=0.52, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 14:  73%|███████▎  | 8/11 [00:00<00:00, 152.01it/s, loss=0.521, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 15:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.523, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:   9%|▉         | 1/11 [00:00<00:00, 126.91it/s, loss=0.526, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:  18%|█▊        | 2/11 [00:00<00:00, 140.33it/s, loss=0.53, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:  27%|██▋       | 3/11 [00:00<00:00, 145.52it/s, loss=0.526, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:  36%|███▋      | 4/11 [00:00<00:00, 151.16it/s, loss=0.52, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:  45%|████▌     | 5/11 [00:00<00:00, 150.84it/s, loss=0.521, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:  55%|█████▍    | 6/11 [00:00<00:00, 153.42it/s, loss=0.52, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:  64%|██████▎   | 7/11 [00:00<00:00, 154.63it/s, loss=0.513, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 15:  73%|███████▎  | 8/11 [00:00<00:00, 155.98it/s, loss=0.514, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 16:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.516, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:   9%|▉         | 1/11 [00:00<00:00, 139.92it/s, loss=0.519, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:  18%|█▊        | 2/11 [00:00<00:00, 145.88it/s, loss=0.522, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:  27%|██▋       | 3/11 [00:00<00:00, 153.76it/s, loss=0.519, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:  36%|███▋      | 4/11 [00:00<00:00, 156.35it/s, loss=0.513, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:  45%|████▌     | 5/11 [00:00<00:00, 156.76it/s, loss=0.513, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:  55%|█████▍    | 6/11 [00:00<00:00, 158.01it/s, loss=0.512, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:  64%|██████▎   | 7/11 [00:00<00:00, 156.99it/s, loss=0.506, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 16:  73%|███████▎  | 8/11 [00:00<00:00, 157.44it/s, loss=0.506, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 17:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.508, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:   9%|▉         | 1/11 [00:00<00:00, 130.02it/s, loss=0.511, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:  18%|█▊        | 2/11 [00:00<00:00, 147.59it/s, loss=0.515, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:  27%|██▋       | 3/11 [00:00<00:00, 148.54it/s, loss=0.511, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:  36%|███▋      | 4/11 [00:00<00:00, 144.22it/s, loss=0.505, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:  45%|████▌     | 5/11 [00:00<00:00, 38.90it/s, loss=0.506, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:  55%|█████▍    | 6/11 [00:00<00:00, 44.26it/s, loss=0.504, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:  64%|██████▎   | 7/11 [00:00<00:00, 49.37it/s, loss=0.498, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 17:  73%|███████▎  | 8/11 [00:00<00:00, 54.19it/s, loss=0.498, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 18:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.5, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:   9%|▉         | 1/11 [00:00<00:00, 125.08it/s, loss=0.503, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:  18%|█▊        | 2/11 [00:00<00:00, 143.34it/s, loss=0.507, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:  27%|██▋       | 3/11 [00:00<00:00, 150.66it/s, loss=0.503, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:  36%|███▋      | 4/11 [00:00<00:00, 149.87it/s, loss=0.497, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:  45%|████▌     | 5/11 [00:00<00:00, 150.88it/s, loss=0.497, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:  55%|█████▍    | 6/11 [00:00<00:00, 152.96it/s, loss=0.496, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:  64%|██████▎   | 7/11 [00:00<00:00, 153.72it/s, loss=0.49, v_num=35] torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 18:  73%|███████▎  | 8/11 [00:00<00:00, 155.78it/s, loss=0.49, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 19:   0%|          | 0/11 [00:00<?, ?it/s, loss=0.491, v_num=35]          torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:   9%|▉         | 1/11 [00:00<00:00, 128.91it/s, loss=0.494, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:  18%|█▊        | 2/11 [00:00<00:00, 142.77it/s, loss=0.498, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:  27%|██▋       | 3/11 [00:00<00:00, 141.13it/s, loss=0.494, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:  36%|███▋      | 4/11 [00:00<00:00, 144.05it/s, loss=0.488, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:  45%|████▌     | 5/11 [00:00<00:00, 147.39it/s, loss=0.489, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:  55%|█████▍    | 6/11 [00:00<00:00, 149.78it/s, loss=0.488, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:  64%|██████▎   | 7/11 [00:00<00:00, 152.60it/s, loss=0.481, v_num=35]torch.Size([1000, 768]) torch.Size([1000, 1])\n",
      "Epoch 19:  73%|███████▎  | 8/11 [00:00<00:00, 154.93it/s, loss=0.482, v_num=35]torch.Size([551, 768]) torch.Size([551, 1])\n",
      "Epoch 19: 100%|██████████| 11/11 [00:00<00:00, 142.06it/s, loss=0.483, v_num=35]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 11/11 [00:00<00:00, 127.80it/s, loss=0.483, v_num=35]\n"
     ]
    }
   ],
   "source": [
    "dm.setup(stage='fit')\n",
    "dm.setup(stage='validate')\n",
    "dm.setup(stage='test')\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=20)\n",
    "trainer.fit(model=cls, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 589.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.6881],\n",
       "         [0.7564],\n",
       "         [0.9387],\n",
       "         [0.7084],\n",
       "         [0.6955],\n",
       "         [0.6842],\n",
       "         [0.7257],\n",
       "         [0.8523],\n",
       "         [0.4178],\n",
       "         [0.8290],\n",
       "         [0.7476],\n",
       "         [0.5684],\n",
       "         [0.5793],\n",
       "         [0.7689],\n",
       "         [0.7537],\n",
       "         [0.7329],\n",
       "         [0.5245],\n",
       "         [0.7752],\n",
       "         [0.3226],\n",
       "         [0.5683],\n",
       "         [0.6753],\n",
       "         [0.9400],\n",
       "         [0.8018],\n",
       "         [0.8396],\n",
       "         [0.6740],\n",
       "         [0.8224],\n",
       "         [0.6328],\n",
       "         [0.4659],\n",
       "         [0.4450],\n",
       "         [0.7123],\n",
       "         [0.7595],\n",
       "         [0.6831],\n",
       "         [0.5121],\n",
       "         [0.5543],\n",
       "         [0.6204],\n",
       "         [0.7741],\n",
       "         [0.6844],\n",
       "         [0.5232],\n",
       "         [0.9160],\n",
       "         [0.6659],\n",
       "         [0.5258],\n",
       "         [0.8834],\n",
       "         [0.7505],\n",
       "         [0.6454],\n",
       "         [0.7164],\n",
       "         [0.4878],\n",
       "         [0.6649],\n",
       "         [0.7732],\n",
       "         [0.5654],\n",
       "         [0.8847],\n",
       "         [0.6966],\n",
       "         [0.6523],\n",
       "         [0.7046],\n",
       "         [0.7322],\n",
       "         [0.6862],\n",
       "         [0.6344],\n",
       "         [0.7306],\n",
       "         [0.9008],\n",
       "         [0.5921],\n",
       "         [0.7158],\n",
       "         [0.7350],\n",
       "         [0.8286],\n",
       "         [0.6569],\n",
       "         [0.8293],\n",
       "         [0.8678],\n",
       "         [0.7801],\n",
       "         [0.8683],\n",
       "         [0.7227],\n",
       "         [0.8906],\n",
       "         [0.6517],\n",
       "         [0.8274],\n",
       "         [0.5924],\n",
       "         [0.5292],\n",
       "         [0.6906],\n",
       "         [0.7277],\n",
       "         [0.8833],\n",
       "         [0.7314],\n",
       "         [0.9138],\n",
       "         [0.4430],\n",
       "         [0.6275],\n",
       "         [0.5353],\n",
       "         [0.3702],\n",
       "         [0.8575],\n",
       "         [0.9551],\n",
       "         [0.7670],\n",
       "         [0.9207],\n",
       "         [0.4593],\n",
       "         [0.9223],\n",
       "         [0.8740],\n",
       "         [0.5726],\n",
       "         [0.6782],\n",
       "         [0.9083],\n",
       "         [0.5259],\n",
       "         [0.7295],\n",
       "         [0.5831],\n",
       "         [0.7698],\n",
       "         [0.7193],\n",
       "         [0.7220],\n",
       "         [0.8406],\n",
       "         [0.9096],\n",
       "         [0.5798],\n",
       "         [0.6303],\n",
       "         [0.4780],\n",
       "         [0.4683],\n",
       "         [0.5606],\n",
       "         [0.8391],\n",
       "         [0.8014],\n",
       "         [0.8860],\n",
       "         [0.7259],\n",
       "         [0.5166],\n",
       "         [0.8360],\n",
       "         [0.5952],\n",
       "         [0.9393],\n",
       "         [0.8752],\n",
       "         [0.8703],\n",
       "         [0.7288],\n",
       "         [0.7586],\n",
       "         [0.6572],\n",
       "         [0.5244],\n",
       "         [0.8443],\n",
       "         [0.8158],\n",
       "         [0.8212],\n",
       "         [0.8056],\n",
       "         [0.6539],\n",
       "         [0.8272],\n",
       "         [0.8096],\n",
       "         [0.3926],\n",
       "         [0.9180],\n",
       "         [0.7062],\n",
       "         [0.6868],\n",
       "         [0.3779],\n",
       "         [0.6969],\n",
       "         [0.8519],\n",
       "         [0.4718],\n",
       "         [0.4698],\n",
       "         [0.9302],\n",
       "         [0.7855],\n",
       "         [0.5852],\n",
       "         [0.7825],\n",
       "         [0.7788],\n",
       "         [0.4201],\n",
       "         [0.7293],\n",
       "         [0.8045],\n",
       "         [0.7174],\n",
       "         [0.9588],\n",
       "         [0.4278],\n",
       "         [0.5645],\n",
       "         [0.7072],\n",
       "         [0.7334],\n",
       "         [0.6148],\n",
       "         [0.7096],\n",
       "         [0.7443],\n",
       "         [0.4917],\n",
       "         [0.6810],\n",
       "         [0.8316],\n",
       "         [0.8084],\n",
       "         [0.6178],\n",
       "         [0.6248],\n",
       "         [0.5126],\n",
       "         [0.5870],\n",
       "         [0.9390],\n",
       "         [0.7209],\n",
       "         [0.6787],\n",
       "         [0.8829],\n",
       "         [0.9044],\n",
       "         [0.5272],\n",
       "         [0.6295],\n",
       "         [0.5275],\n",
       "         [0.4679],\n",
       "         [0.5307],\n",
       "         [0.6480],\n",
       "         [0.8186],\n",
       "         [0.6295],\n",
       "         [0.6029],\n",
       "         [0.8984],\n",
       "         [0.7618],\n",
       "         [0.8339],\n",
       "         [0.7895],\n",
       "         [0.8255],\n",
       "         [0.9283],\n",
       "         [0.8087],\n",
       "         [0.8595],\n",
       "         [0.5281],\n",
       "         [0.6612],\n",
       "         [0.5729],\n",
       "         [0.6779],\n",
       "         [0.9276],\n",
       "         [0.5696],\n",
       "         [0.7551],\n",
       "         [0.7621],\n",
       "         [0.7651],\n",
       "         [0.6135],\n",
       "         [0.7392],\n",
       "         [0.8676],\n",
       "         [0.7364],\n",
       "         [0.9211],\n",
       "         [0.7501],\n",
       "         [0.6431],\n",
       "         [0.9320],\n",
       "         [0.5839],\n",
       "         [0.7748],\n",
       "         [0.6420],\n",
       "         [0.7257],\n",
       "         [0.9411],\n",
       "         [0.8574],\n",
       "         [0.9073],\n",
       "         [0.5933],\n",
       "         [0.8983],\n",
       "         [0.7558],\n",
       "         [0.4730],\n",
       "         [0.9248],\n",
       "         [0.8069],\n",
       "         [0.6800],\n",
       "         [0.6996],\n",
       "         [0.9503],\n",
       "         [0.7730],\n",
       "         [0.8681],\n",
       "         [0.9264],\n",
       "         [0.6843],\n",
       "         [0.4549],\n",
       "         [0.9105],\n",
       "         [0.6677],\n",
       "         [0.8871],\n",
       "         [0.8862],\n",
       "         [0.4063],\n",
       "         [0.9284],\n",
       "         [0.9251],\n",
       "         [0.7423],\n",
       "         [0.2856],\n",
       "         [0.8698],\n",
       "         [0.4289],\n",
       "         [0.9571],\n",
       "         [0.9355],\n",
       "         [0.7995],\n",
       "         [0.4057],\n",
       "         [0.8114],\n",
       "         [0.6710],\n",
       "         [0.8619],\n",
       "         [0.9311],\n",
       "         [0.8105],\n",
       "         [0.8494],\n",
       "         [0.7612],\n",
       "         [0.7861],\n",
       "         [0.7082],\n",
       "         [0.8859],\n",
       "         [0.8614],\n",
       "         [0.9140],\n",
       "         [0.7365],\n",
       "         [0.6312],\n",
       "         [0.9471],\n",
       "         [0.7141],\n",
       "         [0.6276],\n",
       "         [0.7891],\n",
       "         [0.5508],\n",
       "         [0.9595],\n",
       "         [0.9565],\n",
       "         [0.8613],\n",
       "         [0.6241],\n",
       "         [0.6490],\n",
       "         [0.8586],\n",
       "         [0.6925],\n",
       "         [0.7274],\n",
       "         [0.6640],\n",
       "         [0.8602],\n",
       "         [0.5407],\n",
       "         [0.8172],\n",
       "         [0.7875],\n",
       "         [0.6552],\n",
       "         [0.7135],\n",
       "         [0.9111],\n",
       "         [0.5460],\n",
       "         [0.7533],\n",
       "         [0.5816],\n",
       "         [0.9246],\n",
       "         [0.8663],\n",
       "         [0.8588],\n",
       "         [0.7080],\n",
       "         [0.6552],\n",
       "         [0.8523],\n",
       "         [0.5126],\n",
       "         [0.8210],\n",
       "         [0.5403],\n",
       "         [0.9166],\n",
       "         [0.9555],\n",
       "         [0.7341],\n",
       "         [0.3297],\n",
       "         [0.7004],\n",
       "         [0.5685],\n",
       "         [0.8289],\n",
       "         [0.9090],\n",
       "         [0.8184],\n",
       "         [0.5425],\n",
       "         [0.9030],\n",
       "         [0.2814],\n",
       "         [0.9295],\n",
       "         [0.4755],\n",
       "         [0.8534],\n",
       "         [0.7458],\n",
       "         [0.7907],\n",
       "         [0.4893],\n",
       "         [0.9182],\n",
       "         [0.5245],\n",
       "         [0.6707],\n",
       "         [0.7678],\n",
       "         [0.8712],\n",
       "         [0.9267],\n",
       "         [0.4213],\n",
       "         [0.6269],\n",
       "         [0.6755],\n",
       "         [0.8342],\n",
       "         [0.8335],\n",
       "         [0.6557],\n",
       "         [0.4170],\n",
       "         [0.7637],\n",
       "         [0.5517],\n",
       "         [0.5530],\n",
       "         [0.5653],\n",
       "         [0.5137],\n",
       "         [0.8027],\n",
       "         [0.8229],\n",
       "         [0.5447],\n",
       "         [0.6663],\n",
       "         [0.3662],\n",
       "         [0.4879],\n",
       "         [0.6624],\n",
       "         [0.4373],\n",
       "         [0.8745],\n",
       "         [0.7872],\n",
       "         [0.9185],\n",
       "         [0.4838],\n",
       "         [0.7253],\n",
       "         [0.6117],\n",
       "         [0.8897],\n",
       "         [0.8866],\n",
       "         [0.9176],\n",
       "         [0.8927],\n",
       "         [0.7435],\n",
       "         [0.7927],\n",
       "         [0.7377],\n",
       "         [0.7296],\n",
       "         [0.7865],\n",
       "         [0.7956],\n",
       "         [0.7965],\n",
       "         [0.7231],\n",
       "         [0.8539],\n",
       "         [0.4668],\n",
       "         [0.6444],\n",
       "         [0.9198],\n",
       "         [0.7754],\n",
       "         [0.6917],\n",
       "         [0.8893],\n",
       "         [0.4796],\n",
       "         [0.7794],\n",
       "         [0.8908],\n",
       "         [0.5568],\n",
       "         [0.7197],\n",
       "         [0.2970],\n",
       "         [0.7516],\n",
       "         [0.7642],\n",
       "         [0.7214],\n",
       "         [0.8837],\n",
       "         [0.6985],\n",
       "         [0.8656],\n",
       "         [0.4312],\n",
       "         [0.7665],\n",
       "         [0.7073],\n",
       "         [0.8758],\n",
       "         [0.9220],\n",
       "         [0.8037],\n",
       "         [0.8961],\n",
       "         [0.8704],\n",
       "         [0.9052],\n",
       "         [0.9032],\n",
       "         [0.9344],\n",
       "         [0.8894],\n",
       "         [0.9508],\n",
       "         [0.6890],\n",
       "         [0.9123],\n",
       "         [0.5789],\n",
       "         [0.7902],\n",
       "         [0.5452],\n",
       "         [0.8297],\n",
       "         [0.9369],\n",
       "         [0.7039],\n",
       "         [0.8795],\n",
       "         [0.8236],\n",
       "         [0.9231],\n",
       "         [0.8379],\n",
       "         [0.8664],\n",
       "         [0.4960],\n",
       "         [0.8988],\n",
       "         [0.4670],\n",
       "         [0.7187],\n",
       "         [0.8951],\n",
       "         [0.8639],\n",
       "         [0.7617],\n",
       "         [0.7526],\n",
       "         [0.8768],\n",
       "         [0.9341],\n",
       "         [0.8092],\n",
       "         [0.7580],\n",
       "         [0.6496],\n",
       "         [0.9576],\n",
       "         [0.8964],\n",
       "         [0.7246],\n",
       "         [0.7937],\n",
       "         [0.7084],\n",
       "         [0.8545],\n",
       "         [0.5908],\n",
       "         [0.8447],\n",
       "         [0.7775],\n",
       "         [0.8926],\n",
       "         [0.8582],\n",
       "         [0.6304],\n",
       "         [0.9155],\n",
       "         [0.3126],\n",
       "         [0.6550],\n",
       "         [0.5591],\n",
       "         [0.7625],\n",
       "         [0.3585],\n",
       "         [0.8424],\n",
       "         [0.9255],\n",
       "         [0.9240],\n",
       "         [0.9010],\n",
       "         [0.8044],\n",
       "         [0.9734],\n",
       "         [0.8637],\n",
       "         [0.8401],\n",
       "         [0.6517],\n",
       "         [0.9218],\n",
       "         [0.7007],\n",
       "         [0.6063],\n",
       "         [0.7408],\n",
       "         [0.7608],\n",
       "         [0.9073],\n",
       "         [0.8186],\n",
       "         [0.7347],\n",
       "         [0.8305],\n",
       "         [0.7309],\n",
       "         [0.8609],\n",
       "         [0.8487],\n",
       "         [0.7955],\n",
       "         [0.9611],\n",
       "         [0.6694],\n",
       "         [0.5094],\n",
       "         [0.5728],\n",
       "         [0.7836],\n",
       "         [0.6088],\n",
       "         [0.6162],\n",
       "         [0.6408],\n",
       "         [0.5776],\n",
       "         [0.8561],\n",
       "         [0.6389],\n",
       "         [0.8226],\n",
       "         [0.7778],\n",
       "         [0.7186],\n",
       "         [0.7910],\n",
       "         [0.9350],\n",
       "         [0.8137],\n",
       "         [0.3540],\n",
       "         [0.7488],\n",
       "         [0.7373],\n",
       "         [0.4870],\n",
       "         [0.6059],\n",
       "         [0.6602],\n",
       "         [0.4523],\n",
       "         [0.8400],\n",
       "         [0.9071],\n",
       "         [0.8213],\n",
       "         [0.8226],\n",
       "         [0.6803],\n",
       "         [0.6464],\n",
       "         [0.6755],\n",
       "         [0.4056],\n",
       "         [0.6001],\n",
       "         [0.4350],\n",
       "         [0.6079],\n",
       "         [0.4389],\n",
       "         [0.7957],\n",
       "         [0.5099],\n",
       "         [0.8700],\n",
       "         [0.6376],\n",
       "         [0.7290],\n",
       "         [0.5093],\n",
       "         [0.4949],\n",
       "         [0.6030],\n",
       "         [0.8157],\n",
       "         [0.6164],\n",
       "         [0.3195],\n",
       "         [0.5763],\n",
       "         [0.6106],\n",
       "         [0.3905],\n",
       "         [0.8171],\n",
       "         [0.6701],\n",
       "         [0.6672],\n",
       "         [0.7354],\n",
       "         [0.2954],\n",
       "         [0.7875],\n",
       "         [0.8923],\n",
       "         [0.8179],\n",
       "         [0.8965],\n",
       "         [0.9590],\n",
       "         [0.8633],\n",
       "         [0.8876],\n",
       "         [0.2118],\n",
       "         [0.4747],\n",
       "         [0.6656],\n",
       "         [0.8599],\n",
       "         [0.7199],\n",
       "         [0.8763],\n",
       "         [0.7030],\n",
       "         [0.5949],\n",
       "         [0.8287],\n",
       "         [0.7752],\n",
       "         [0.6838],\n",
       "         [0.7473],\n",
       "         [0.6391],\n",
       "         [0.6208],\n",
       "         [0.8607],\n",
       "         [0.7221],\n",
       "         [0.6429],\n",
       "         [0.8641],\n",
       "         [0.8165],\n",
       "         [0.9096],\n",
       "         [0.3065],\n",
       "         [0.8225],\n",
       "         [0.8226],\n",
       "         [0.6191],\n",
       "         [0.7055],\n",
       "         [0.6412],\n",
       "         [0.6962],\n",
       "         [0.6817],\n",
       "         [0.8320],\n",
       "         [0.8910],\n",
       "         [0.5595],\n",
       "         [0.6307],\n",
       "         [0.8368],\n",
       "         [0.7611],\n",
       "         [0.7718],\n",
       "         [0.8276],\n",
       "         [0.6122],\n",
       "         [0.7671],\n",
       "         [0.7129],\n",
       "         [0.5121],\n",
       "         [0.8267],\n",
       "         [0.7634],\n",
       "         [0.6774],\n",
       "         [0.7265],\n",
       "         [0.5376],\n",
       "         [0.8990],\n",
       "         [0.7594],\n",
       "         [0.5660],\n",
       "         [0.5282],\n",
       "         [0.5772],\n",
       "         [0.8326],\n",
       "         [0.6356],\n",
       "         [0.8569],\n",
       "         [0.8914],\n",
       "         [0.8056],\n",
       "         [0.9171],\n",
       "         [0.7171],\n",
       "         [0.7539],\n",
       "         [0.8726],\n",
       "         [0.8767],\n",
       "         [0.8937],\n",
       "         [0.7363],\n",
       "         [0.6365],\n",
       "         [0.8948],\n",
       "         [0.9318],\n",
       "         [0.8422],\n",
       "         [0.6960],\n",
       "         [0.6730],\n",
       "         [0.7882],\n",
       "         [0.8369],\n",
       "         [0.9044],\n",
       "         [0.7855],\n",
       "         [0.8283],\n",
       "         [0.7996],\n",
       "         [0.8283],\n",
       "         [0.7996],\n",
       "         [0.6906],\n",
       "         [0.6697],\n",
       "         [0.8473],\n",
       "         [0.8751],\n",
       "         [0.8008],\n",
       "         [0.5623],\n",
       "         [0.7305],\n",
       "         [0.8939],\n",
       "         [0.8208],\n",
       "         [0.7055],\n",
       "         [0.6110],\n",
       "         [0.6245],\n",
       "         [0.4273],\n",
       "         [0.8815],\n",
       "         [0.5991],\n",
       "         [0.8655],\n",
       "         [0.9332],\n",
       "         [0.8824],\n",
       "         [0.7828],\n",
       "         [0.8292],\n",
       "         [0.8261],\n",
       "         [0.8292],\n",
       "         [0.7040],\n",
       "         [0.8035],\n",
       "         [0.4914],\n",
       "         [0.5790],\n",
       "         [0.5850],\n",
       "         [0.7537],\n",
       "         [0.7067],\n",
       "         [0.7254],\n",
       "         [0.7433],\n",
       "         [0.7258],\n",
       "         [0.8790],\n",
       "         [0.7773],\n",
       "         [0.7371],\n",
       "         [0.5001],\n",
       "         [0.8943],\n",
       "         [0.5611],\n",
       "         [0.8328],\n",
       "         [0.9154],\n",
       "         [0.7027],\n",
       "         [0.4456],\n",
       "         [0.6731],\n",
       "         [0.8908],\n",
       "         [0.7055],\n",
       "         [0.7795],\n",
       "         [0.8162],\n",
       "         [0.9175],\n",
       "         [0.7777],\n",
       "         [0.8652],\n",
       "         [0.8615],\n",
       "         [0.7860],\n",
       "         [0.6640],\n",
       "         [0.9058],\n",
       "         [0.7588],\n",
       "         [0.4530],\n",
       "         [0.6168],\n",
       "         [0.7575],\n",
       "         [0.9103],\n",
       "         [0.9269],\n",
       "         [0.8233],\n",
       "         [0.6258],\n",
       "         [0.4940],\n",
       "         [0.6621],\n",
       "         [0.9051],\n",
       "         [0.8174],\n",
       "         [0.7680],\n",
       "         [0.9505],\n",
       "         [0.7046],\n",
       "         [0.7600],\n",
       "         [0.9320],\n",
       "         [0.5208],\n",
       "         [0.7953],\n",
       "         [0.5652],\n",
       "         [0.8027],\n",
       "         [0.6983],\n",
       "         [0.5403],\n",
       "         [0.5778],\n",
       "         [0.9556],\n",
       "         [0.9641],\n",
       "         [0.8018],\n",
       "         [0.8186],\n",
       "         [0.7735],\n",
       "         [0.7887],\n",
       "         [0.5931],\n",
       "         [0.8901],\n",
       "         [0.7893],\n",
       "         [0.9190],\n",
       "         [0.8600],\n",
       "         [0.7158],\n",
       "         [0.8243],\n",
       "         [0.6547],\n",
       "         [0.6885],\n",
       "         [0.6649],\n",
       "         [0.5901],\n",
       "         [0.8696],\n",
       "         [0.9112],\n",
       "         [0.5073],\n",
       "         [0.5558],\n",
       "         [0.9067],\n",
       "         [0.8633],\n",
       "         [0.8092],\n",
       "         [0.4746],\n",
       "         [0.9464],\n",
       "         [0.6294],\n",
       "         [0.7902],\n",
       "         [0.8274],\n",
       "         [0.8049],\n",
       "         [0.8004],\n",
       "         [0.4958],\n",
       "         [0.7472],\n",
       "         [0.5639],\n",
       "         [0.3769],\n",
       "         [0.5112],\n",
       "         [0.8399],\n",
       "         [0.6508],\n",
       "         [0.7826],\n",
       "         [0.9025],\n",
       "         [0.8667],\n",
       "         [0.6643],\n",
       "         [0.7877],\n",
       "         [0.6934],\n",
       "         [0.6750],\n",
       "         [0.8825],\n",
       "         [0.8168],\n",
       "         [0.5122],\n",
       "         [0.4240],\n",
       "         [0.4773],\n",
       "         [0.6789],\n",
       "         [0.6993],\n",
       "         [0.6222],\n",
       "         [0.8173],\n",
       "         [0.8058],\n",
       "         [0.8510],\n",
       "         [0.6715],\n",
       "         [0.7206],\n",
       "         [0.5318],\n",
       "         [0.5203],\n",
       "         [0.6028],\n",
       "         [0.5211],\n",
       "         [0.5531],\n",
       "         [0.9046],\n",
       "         [0.6876],\n",
       "         [0.7435],\n",
       "         [0.7364],\n",
       "         [0.7196],\n",
       "         [0.4792],\n",
       "         [0.6078],\n",
       "         [0.8020],\n",
       "         [0.8933],\n",
       "         [0.7121],\n",
       "         [0.5699],\n",
       "         [0.8263],\n",
       "         [0.5870],\n",
       "         [0.7128],\n",
       "         [0.5534],\n",
       "         [0.5869],\n",
       "         [0.9485],\n",
       "         [0.8368],\n",
       "         [0.8441],\n",
       "         [0.6026],\n",
       "         [0.4160],\n",
       "         [0.6549],\n",
       "         [0.8526],\n",
       "         [0.7297],\n",
       "         [0.6078],\n",
       "         [0.4910],\n",
       "         [0.3991],\n",
       "         [0.7672],\n",
       "         [0.7297],\n",
       "         [0.5939],\n",
       "         [0.9749],\n",
       "         [0.9778],\n",
       "         [0.8712],\n",
       "         [0.8751],\n",
       "         [0.4816],\n",
       "         [0.5047],\n",
       "         [0.7415],\n",
       "         [0.6774],\n",
       "         [0.7366],\n",
       "         [0.7758],\n",
       "         [0.8285],\n",
       "         [0.9068],\n",
       "         [0.5665],\n",
       "         [0.7962],\n",
       "         [0.6141],\n",
       "         [0.5291],\n",
       "         [0.7694],\n",
       "         [0.9072],\n",
       "         [0.8604],\n",
       "         [0.7908],\n",
       "         [0.6725],\n",
       "         [0.6591],\n",
       "         [0.6437],\n",
       "         [0.7995],\n",
       "         [0.6734],\n",
       "         [0.7317],\n",
       "         [0.8820],\n",
       "         [0.6153],\n",
       "         [0.9252],\n",
       "         [0.8545],\n",
       "         [0.9415],\n",
       "         [0.9749],\n",
       "         [0.5143],\n",
       "         [0.8330],\n",
       "         [0.8768],\n",
       "         [0.6118],\n",
       "         [0.7814],\n",
       "         [0.8690],\n",
       "         [0.8143],\n",
       "         [0.6904],\n",
       "         [0.7666],\n",
       "         [0.6224],\n",
       "         [0.5954],\n",
       "         [0.7555],\n",
       "         [0.6752],\n",
       "         [0.7050],\n",
       "         [0.8644],\n",
       "         [0.7546],\n",
       "         [0.5342],\n",
       "         [0.9120],\n",
       "         [0.7917],\n",
       "         [0.8302],\n",
       "         [0.5948],\n",
       "         [0.8124],\n",
       "         [0.6066],\n",
       "         [0.8829],\n",
       "         [0.7111],\n",
       "         [0.6558],\n",
       "         [0.8779],\n",
       "         [0.8194],\n",
       "         [0.8527],\n",
       "         [0.8128],\n",
       "         [0.5070],\n",
       "         [0.6902],\n",
       "         [0.3585],\n",
       "         [0.6948],\n",
       "         [0.6728],\n",
       "         [0.9185],\n",
       "         [0.5850],\n",
       "         [0.7051],\n",
       "         [0.4470],\n",
       "         [0.4890],\n",
       "         [0.4027],\n",
       "         [0.9625],\n",
       "         [0.7647],\n",
       "         [0.8911],\n",
       "         [0.9453],\n",
       "         [0.8460],\n",
       "         [0.6290],\n",
       "         [0.5346],\n",
       "         [0.5199],\n",
       "         [0.4714],\n",
       "         [0.8989],\n",
       "         [0.7806],\n",
       "         [0.7556],\n",
       "         [0.8768],\n",
       "         [0.7855],\n",
       "         [0.8297],\n",
       "         [0.8602],\n",
       "         [0.6971],\n",
       "         [0.7695],\n",
       "         [0.5765],\n",
       "         [0.5840],\n",
       "         [0.5605],\n",
       "         [0.5020],\n",
       "         [0.3297],\n",
       "         [0.9027],\n",
       "         [0.8202],\n",
       "         [0.8604],\n",
       "         [0.8629],\n",
       "         [0.9605],\n",
       "         [0.6781],\n",
       "         [0.4959],\n",
       "         [0.7985],\n",
       "         [0.7187],\n",
       "         [0.7597],\n",
       "         [0.9318],\n",
       "         [0.8456],\n",
       "         [0.9191],\n",
       "         [0.8499],\n",
       "         [0.6456],\n",
       "         [0.8153],\n",
       "         [0.7018],\n",
       "         [0.5750],\n",
       "         [0.5299],\n",
       "         [0.6047],\n",
       "         [0.7788],\n",
       "         [0.7862],\n",
       "         [0.8474],\n",
       "         [0.6124],\n",
       "         [0.8270],\n",
       "         [0.3730],\n",
       "         [0.3500],\n",
       "         [0.8651],\n",
       "         [0.8367],\n",
       "         [0.8367],\n",
       "         [0.4238],\n",
       "         [0.6658],\n",
       "         [0.6667],\n",
       "         [0.5843],\n",
       "         [0.9473],\n",
       "         [0.8433],\n",
       "         [0.9355],\n",
       "         [0.6877],\n",
       "         [0.7030],\n",
       "         [0.6979],\n",
       "         [0.5363],\n",
       "         [0.3128],\n",
       "         [0.4443],\n",
       "         [0.3311],\n",
       "         [0.6109],\n",
       "         [0.6377],\n",
       "         [0.6257],\n",
       "         [0.9182],\n",
       "         [0.3871],\n",
       "         [0.7634],\n",
       "         [0.8855],\n",
       "         [0.2326],\n",
       "         [0.3118],\n",
       "         [0.8713],\n",
       "         [0.8660],\n",
       "         [0.8055],\n",
       "         [0.8449],\n",
       "         [0.7025],\n",
       "         [0.8587],\n",
       "         [0.8458],\n",
       "         [0.8459],\n",
       "         [0.7762],\n",
       "         [0.9200],\n",
       "         [0.6957],\n",
       "         [0.5716],\n",
       "         [0.9649],\n",
       "         [0.9580],\n",
       "         [0.8292],\n",
       "         [0.9515],\n",
       "         [0.8549],\n",
       "         [0.8371],\n",
       "         [0.8622],\n",
       "         [0.8984],\n",
       "         [0.8424],\n",
       "         [0.8112],\n",
       "         [0.6902],\n",
       "         [0.8159],\n",
       "         [0.6983],\n",
       "         [0.9530],\n",
       "         [0.8959],\n",
       "         [0.6620],\n",
       "         [0.6710],\n",
       "         [0.7772],\n",
       "         [0.7596],\n",
       "         [0.7973],\n",
       "         [0.3872],\n",
       "         [0.4473],\n",
       "         [0.7964],\n",
       "         [0.7537],\n",
       "         [0.8119],\n",
       "         [0.6087],\n",
       "         [0.8180],\n",
       "         [0.6222],\n",
       "         [0.9481],\n",
       "         [0.6569],\n",
       "         [0.7948],\n",
       "         [0.6064],\n",
       "         [0.9005],\n",
       "         [0.6407],\n",
       "         [0.6521],\n",
       "         [0.7918],\n",
       "         [0.5642],\n",
       "         [0.4472],\n",
       "         [0.5943],\n",
       "         [0.9105],\n",
       "         [0.6355],\n",
       "         [0.3844],\n",
       "         [0.8466],\n",
       "         [0.7334],\n",
       "         [0.9087],\n",
       "         [0.8637],\n",
       "         [0.9043],\n",
       "         [0.6738],\n",
       "         [0.6225],\n",
       "         [0.9362],\n",
       "         [0.9046],\n",
       "         [0.9364],\n",
       "         [0.9239],\n",
       "         [0.8861],\n",
       "         [0.8983],\n",
       "         [0.7285],\n",
       "         [0.8172],\n",
       "         [0.7195],\n",
       "         [0.7072],\n",
       "         [0.7780],\n",
       "         [0.7595],\n",
       "         [0.8515],\n",
       "         [0.8668],\n",
       "         [0.8744],\n",
       "         [0.6314],\n",
       "         [0.7710],\n",
       "         [0.5387],\n",
       "         [0.8189],\n",
       "         [0.8889],\n",
       "         [0.8397],\n",
       "         [0.4211],\n",
       "         [0.7421],\n",
       "         [0.4852],\n",
       "         [0.6671],\n",
       "         [0.5651],\n",
       "         [0.6725],\n",
       "         [0.5162],\n",
       "         [0.7219],\n",
       "         [0.7416],\n",
       "         [0.4395],\n",
       "         [0.7804],\n",
       "         [0.6910],\n",
       "         [0.6745],\n",
       "         [0.5867],\n",
       "         [0.5778],\n",
       "         [0.6422],\n",
       "         [0.5237],\n",
       "         [0.7715]]),\n",
       " tensor([[0.9266],\n",
       "         [0.5182],\n",
       "         [0.6596],\n",
       "         [0.8049],\n",
       "         [0.2982],\n",
       "         [0.9113],\n",
       "         [0.7284],\n",
       "         [0.7463],\n",
       "         [0.9220],\n",
       "         [0.6349],\n",
       "         [0.6316],\n",
       "         [0.8191],\n",
       "         [0.9049],\n",
       "         [0.9120],\n",
       "         [0.9114],\n",
       "         [0.7379],\n",
       "         [0.8472],\n",
       "         [0.5986],\n",
       "         [0.7274],\n",
       "         [0.8582],\n",
       "         [0.8828],\n",
       "         [0.6404],\n",
       "         [0.5632],\n",
       "         [0.3951],\n",
       "         [0.4625],\n",
       "         [0.7010],\n",
       "         [0.4934],\n",
       "         [0.8807],\n",
       "         [0.7890],\n",
       "         [0.7969],\n",
       "         [0.5318],\n",
       "         [0.7531],\n",
       "         [0.8208],\n",
       "         [0.8811],\n",
       "         [0.4768],\n",
       "         [0.8224],\n",
       "         [0.5182],\n",
       "         [0.6250],\n",
       "         [0.6623],\n",
       "         [0.8578],\n",
       "         [0.7026],\n",
       "         [0.4797],\n",
       "         [0.5957],\n",
       "         [0.5483],\n",
       "         [0.6482],\n",
       "         [0.5951],\n",
       "         [0.8412],\n",
       "         [0.7040],\n",
       "         [0.3334],\n",
       "         [0.3302],\n",
       "         [0.7572],\n",
       "         [0.6462],\n",
       "         [0.2957],\n",
       "         [0.6895],\n",
       "         [0.5504],\n",
       "         [0.5426],\n",
       "         [0.8347],\n",
       "         [0.6954],\n",
       "         [0.7235],\n",
       "         [0.7960],\n",
       "         [0.7349],\n",
       "         [0.6388],\n",
       "         [0.7880]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(model=cls, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b020ec3df0c18b68bf7b4552e70845f81858dcbaf541b576d2fc743ffe38c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
