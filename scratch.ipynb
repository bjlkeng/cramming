{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.dataloaders import GLUEDataModule\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Current device: 0\n",
      "Device: cuda:0\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\") \n",
    "device_count = torch.cuda.device_count() \n",
    "device_name =  torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f'Cuda available: {available}')\n",
    "print(f'Current device: {curr_device}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Device count: {device_count}')\n",
    "print(f'Device name: {device_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 28602/28602 [00:03<00:00, 8734.19it/s]  \n"
     ]
    }
   ],
   "source": [
    "from src.dataloaders import BertDataModule\n",
    "\n",
    "files = [\n",
    "'/home/bjlkeng/devel/cramming/data/data/pg19_train/*',\n",
    "#'/home/bjlkeng/devel/cramming/data/data/wikipedia/flat/*',\n",
    "#'/home/bjlkeng/devel/cramming/data/data/books3/books3/the-eye.eu/public/Books/Bibliotik/**',\n",
    "]\n",
    "\n",
    "dm = BertDataModule(source_files=files, tokenizer_name='bert-base-uncased', max_seq_length=128)\n",
    "dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "dataloader = dm.train_dataloader()\n",
    "d = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d['attention_mask'].shape)\n",
    "d['attention_mask'][:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   103,  2214,  9025,  1997,  1996,  2332,  2508,  2544,  1997,\n",
       "          1996,  6331,  1996,  2034, 27513,  1997,  9952,  1024,  2170, 11046,\n",
       "          1015,  1024,   103,   103,  1996,  2927,  2643,  2580,  1996,  6014,\n",
       "          1998,  1996,  3011,   103,  1015,  1024,  1016,  1998,  1996,  3011,\n",
       "          2001,  2302,  2433,  1010,  1998, 11675,  1025,  1998,   103,  2001,\n",
       "          2588,  1996,  2227,  1997,  1996,  2784,  1012,  1998,  1996,  4382,\n",
       "          1997,  2643,  2333,  2588,  1996,  2227,  1997,  1996,   103,  1012,\n",
       "          1015,  1024,  1017,  1998,  2643,  2056,  1010,  2292,   103,  2022,\n",
       "           103,  1024,  1998,   103,  2001,  2422,  1012,  1015,  1024,  1018,\n",
       "          1998,  2643,  2387,  1996,  2422,  1010,  2008,  2009,  2001,  2204,\n",
       "          1024,  1998,  2643,  4055,  1996,   103,   103,  1996,   103,  1012,\n",
       "          1015,  1024,  1019,  1998,  2643, 10464,  1996,  2422,  2154,  1010,\n",
       "          1998,  1996,  4768,   103,  2170,  2305,  1012,   102],\n",
       "        [  101,  1998,  1996,  3944,  1998,  1996,  2851,   103,  1996,  2034,\n",
       "          2154,  1012,  1015,  1024,  1020,  1998,  2643,   103,  1010,  2292,\n",
       "          2045,  2022,  1037,  3813,   103,  1999,  1996, 12930,  1997,  1996,\n",
       "          5380,  1010,  1998,  2292,   103, 11443,  1996,  5380,  2013,  1996,\n",
       "          5380,  1012,  1015,  1024,  1021,  1998,  2643,  2081,  1996,  3813,\n",
       "         24996,  1010,   103,  4055,   103,  5380,  2029,  2020,  2104,  1996,\n",
       "          3813, 24996,  2013,  1996,  5380,  2029,  2020,  2682,  1996,  3813,\n",
       "         24996,  1024,  1998,   103,  2001,  2061,  1012,  1015,  1024,  1022,\n",
       "          1998,  2643,  2170,  1996,  3813, 24996,  6014,  1012,  1998,  1996,\n",
       "          3944, 13082,  1996,  2851,  2020,  1996,  2117,  2154,   103,  1015,\n",
       "           103,  1023,  1998,  2643,  2056,  1010,  2292,  1996,   103,  2104,\n",
       "          1996,  6014,  2022,   103,  2362,   103,  2028,   103,  1010,  1998,\n",
       "          2292,  1996,  4318,  2455,  3711,  1024,  1998,   102],\n",
       "        [  101,  2009,  2001,  2061,  1012,  1015,  1024,  2184,  1998,  2643,\n",
       "           103,  1996,  4318,  2455,  3011,  1025,  1998,  1996,  7215,  2362,\n",
       "          1997,   103,  5380,  2170,  2002, 11915,  1024,  1998,   103,   103,\n",
       "          2008,  2009,   103,   103,  1012,  1015,   103,  2340,   103,  2643,\n",
       "          2056,  1010,  2292,  1996,  3011,  3288,  5743,  5568,   103,  1996,\n",
       "         12810, 21336,  6534,  1010,  1998,  1996,  5909,  3392, 21336,  5909,\n",
       "          2044,   103,  2785,   103,  3005,  6534,  2003,  1999,  2993,  1010,\n",
       "           103,   103,  3011,  1024,  1998,   103,  2001,  2061,   103,  1015,\n",
       "          1024,   103,  1998,  1996,  3011,  2716,  5743,  5568,  1010,   103,\n",
       "         12810, 21336,  6534,  2044,  2010,  2785,  1010,  1998,  1996,  3392,\n",
       "         21336,  5909,  1010,  3005,  6534,  2001,  1999,  2993,  1010,  2044,\n",
       "           103,  2785,  1024,  1998,   103,  2387,  2008,  2009,  2001,  2204,\n",
       "          1012,  1015,  1024,  2410,  1998,  1996,  3944,   102],\n",
       "        [  101,   103,  1996,  2851,  2020,  1996,  2353,  2154,  1012,  1015,\n",
       "          1024,  2403,  1998,  2643,  2056,  1010,   103,  2045,   103,  4597,\n",
       "          1999,  1996,  3813,   103,  1997,  1996,   103,  2000, 11443,  1996,\n",
       "          2154,  2013,  1996,  2305,  6266,  1998,  2292,  2068,   103,  2005,\n",
       "          5751,  1010,  1998,  2005,  3692,  1010,  1998,  2005,  2420,  1010,\n",
       "          1998,  2086,  1024,  1015,  1024,   103,  1998,  2292,   103,  2022,\n",
       "          2005, 21860,  1999,  1996,  3813, 24996,  1997,  1996,  6014,  2000,\n",
       "           103,  2422,  2588,  1996,  3011,  1024,  1998,  2009,  2001,  2061,\n",
       "          1012,  1015,  1024,  2385,  1998,  2643,  2081,  2048,  2307,  4597,\n",
       "          1025,   103,  3618,   103,  2000,  3627,  1996,  2154,  1010,  1998,\n",
       "          1996,  8276,   103,   103,  3627,  1996,  2305,  1024,  2002,  2081,\n",
       "          1996,  3340,  2036,   103,  1015,  1024,  2459,  1998,  2643,   103,\n",
       "          2068,  1999,  1996,  3813, 24996,  1997,  1996,   102],\n",
       "        [  101,  6014,  2000,  2507,  2422,   103,  1996,  3011,  1010,  1015,\n",
       "          1024,   103,  1998,  2000,  3627,  2058,   103,  2154,  1998,  2058,\n",
       "          1996,  2305,  1010,   103,  2000, 11443,  1996,  2422,  2013,  1996,\n",
       "          4768,  1024,  1998,  2643,  2387,  2008,  2009,  2001,   103,   103,\n",
       "          1015,  1024,  2539,  1998,  1996,  3944,  1998,  1996,  2851,  2020,\n",
       "          1996,  2959,  2154,  1012,  1015,  1024,  2322,   103,   103,  2056,\n",
       "          1010,  2292,   103,  5380,  3288,  5743, 12990,  2135,   103,  3048,\n",
       "          6492,  2008,  6045,   103,  2166,  1010,  1998,  1042,  5004,  2140,\n",
       "          2008,  2089,  4875,  2682,  1996,  3011,  1999,  1996,  2330,  3813,\n",
       "         24996,   103,  6014,  1012,  1015,  1024,  2538,  1998,  2643,  2580,\n",
       "          2307, 17967,  1010,  1998,  2296,  2542,  6492,  2008,  2693,  2705,\n",
       "           103,  2029,  1996,  5380,   103,  5743, 12990,   103,  1010,  2044,\n",
       "          2037,  2785,  1010,  1998,  2296, 14462,  1042,   102]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d['input_ids'].shape)\n",
    "d['input_ids'][:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  1996,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  2338,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  1015,  1999,  -100,  -100,  -100,  -100,  1996,  -100,\n",
       "          -100,  -100,  -100,  1012,  -100,  -100,  1016,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  4768,  -100,\n",
       "          2588,  1996,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  5380,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2045,  -100,\n",
       "          2422,  -100,  -100,  2045,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  2422,  2013,  -100,  4768,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  2170,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  2002,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2020,  -100,  -100,\n",
       "          -100,  1012,  -100,  -100,  -100,  -100,  -100,  2056,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100, 24996,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  2009,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1010,  1998,  -100,  1996,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  2009,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1998,  -100,  -100,  -100,  -100,  -100,  -100,  1012,  -100,\n",
       "          1024,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  5380,  -100,\n",
       "          -100,  -100,  -100,  5935,  -100, 19662,  -100,  2173,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          2170,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1996,  -100,  -100,  -100,  -100,  1024,  -100,  2643,  2387,\n",
       "          -100,  -100,  2001,  2204,  -100,  -100,  1024,  -100,  1998,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  2010,  -100,  1010,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          2588,  1996,  -100,  -100,  -100,  2009,  -100,  -100,  1012,  -100,\n",
       "          -100,  2260,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1998,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          2010,  -100,  -100,  -100,  2643,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  1998,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  2292,  -100,  2022,  -100,\n",
       "          -100,  -100,  -100, 24996,  -100,  -100,  6014,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  1025,  -100,  -100,  -100,  2022,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1998,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  2321,  -100,  -100,  2068,  -100,\n",
       "          -100,  4597,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          2507,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1996,  -100,  2422,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  2422,  2000,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,  2275,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  2000,  -100,  -100,  2588,  -100,  -100,  -100,  -100,\n",
       "          -100,  2324,  -100,  -100,  -100,  -100,  1996,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  1998,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2204,  1012,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1998,  2643,  -100,\n",
       "          -100,  -100,  1996,  -100,  -100,  -100,  -100,  -100,  1996,  -100,\n",
       "          6492,  -100,  -100,  2232,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1997,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          1010,  -100,  -100,  -100,  2716,  -100,  -100,  2135,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d['labels'].shape)\n",
    "d['labels'][:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 batches in 83.19 seconds (120.19 batches/s)\n"
     ]
    }
   ],
   "source": [
    "N = 10000 - 1\n",
    "import time\n",
    "start = time.time()\n",
    "for i, d in enumerate(dataloader):\n",
    "    if i >= N:\n",
    "        break\n",
    "end = time.time()\n",
    "print(f'{N} batches in {end-start:.2f} seconds ({N/(end-start):.2f} batches/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b020ec3df0c18b68bf7b4552e70845f81858dcbaf541b576d2fc743ffe38c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
