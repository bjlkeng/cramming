{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.models import SelfAttention, TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Current device: 0\n",
      "Device: cuda:0\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\") \n",
    "device_count = torch.cuda.device_count() \n",
    "device_name =  torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f'Cuda available: {available}')\n",
    "print(f'Current device: {curr_device}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Device count: {device_count}')\n",
    "print(f'Device name: {device_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9147, 1.4812, 1.2380, 1.6248],\n",
       "         [0.5050, 1.8656, 0.1772, 0.9409]],\n",
       "\n",
       "        [[1.2372, 1.0566, 0.9481, 1.8722],\n",
       "         [1.7367, 0.7827, 1.7367, 1.1785]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 2, 4) * 2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3952,  0.6203, -0.0451,  0.1298],\n",
       "         [-0.3948,  0.6122, -0.0486,  0.1234]],\n",
       "\n",
       "        [[-0.4051,  0.5366, -0.1203,  0.0498],\n",
       "         [-0.4057,  0.5405, -0.1191,  0.0528]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfAttention(4, 2)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6598, -0.8952, -0.6573, -0.1074],\n",
       "         [ 0.2433,  1.5264, -0.7353, -1.0344]],\n",
       "\n",
       "        [[ 1.4367,  0.0652, -1.3849, -0.1169],\n",
       "         [ 1.6778, -0.1542, -0.7494, -0.7743]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerBlock(4, 2, 8)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/bjlkeng/devel/cramming/.conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n",
      "Found cached dataset glue (/home/bjlkeng/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 358.93it/s]\n",
      "Loading cached processed dataset at /home/bjlkeng/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1afe93c2c61c7931.arrow\n",
      "Loading cached processed dataset at /home/bjlkeng/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1f28bd522d35d185.arrow\n",
      "Loading cached processed dataset at /home/bjlkeng/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a8f0d4e0f4309ddf.arrow\n",
      "                                                    \r"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from src.dataloaders import GLUEDataModule\n",
    "\n",
    "seed_everything(42)\n",
    "dm = GLUEDataModule(model_name_or_path='bert-large-uncased', \n",
    "                    task_name='cola',\n",
    "                    train_batch_size=32,\n",
    "                    eval_batch_size=32)\n",
    "dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerModule(nn.Module):\n",
    "    def __init__(self, vocab_size, n_blocks, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(d_model, n_heads, d_ff, dropout=dropout) for _ in range(n_blocks)]\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded_text = self.embeddings(text)\n",
    "        embedded_text = self.layer_norm(embedded_text)\n",
    "        transformer_output = self.transformer_blocks(embedded_text)\n",
    "        pooled_output = transformer_output.mean(axis=1)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "\n",
    "class TestModel(LightningModule):\n",
    "    def __init__(self, vocab_size, n_blocks, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model = TransformerModule(vocab_size, n_blocks, d_model, n_heads, d_ff, dropout=dropout)\n",
    "        self.training_step_outputs = []\n",
    "\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(kwargs['input_ids'])\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, batch['labels'].float())\n",
    "        self.training_step_outputs.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss = F.binary_cross_entropy_with_logits(outputs, batch['labels'].float())\n",
    "        return {'loss': val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''Prepare optimizer and schedule (linear warmup and decay)'''\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.0001, betas=(0.9, 0.999),)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset glue (/home/bjlkeng/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1065.54it/s]\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Found cached dataset glue (/home/bjlkeng/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1142.76it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | TransformerModule | 51.8 M\n",
      "--------------------------------------------\n",
      "51.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.8 M    Total params\n",
      "207.142   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 301/301 [00:14<00:00, 20.36it/s, loss=0.554, v_num=25, val_loss=0.716, train_loss=0.523]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 301/301 [00:15<00:00, 19.53it/s, loss=0.554, v_num=25, val_loss=0.716, train_loss=0.523]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "model = TestModel(\n",
    "    vocab_size=dm.tokenizer.vocab_size, \n",
    "    n_blocks=12,\n",
    "    d_model=768,\n",
    "    n_heads=12,\n",
    "    d_ff=768*4,\n",
    "    dropout=0.0\n",
    " )\n",
    "\n",
    "# BK: Using val_loss to pick best model for simplicity here\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator='auto',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b020ec3df0c18b68bf7b4552e70845f81858dcbaf541b576d2fc743ffe38c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
